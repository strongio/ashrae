{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ashrae import DATA_DIR, PROJECT_ROOT, meter_mapping\n",
    "from ashrae.nn import MultiSeriesStateNN, TimeSeriesStateNN\n",
    "from ashrae.preprocessing import DataFrameScaler\n",
    "from ashrae.forecaster import Forecaster\n",
    "\n",
    "try:\n",
    "    from plotnine import *\n",
    "    from IPython.display import clear_output\n",
    "except ImportError:\n",
    "    from ashrae.fake_plotnine import *\n",
    "    clear_output = lambda wait: None\n",
    "    \n",
    "from tqdm import tqdm_notebook    \n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import zip_longest\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch_kalman.kalman_filter import KalmanFilter\n",
    "from torch_kalman.process import LocalLevel, FourierSeasonDynamic, NN\n",
    "from torch_kalman.utils.data import TimeSeriesDataset\n",
    "\n",
    "torch.manual_seed(2019-12-12)\n",
    "np.random.seed(2019-12-12)\n",
    "rs = np.random.RandomState(2019-12-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METER_TYPE = os.environ.get(\"METER_TYPE\", \"steam\")\n",
    "\n",
    "NN_PRETRAIN_NUM_EPOCHS = os.environ.get(\"NN_PRETRAIN_NUM_EPOCHS\", 200 if METER_TYPE=='electricity' else 100 )\n",
    "NN_PRETRAIN_LR = os.environ.get(\"NN_PRETRAIN_LR\", .002 if METER_TYPE=='electricity' else .001 )\n",
    "\n",
    "NN_DROP_NUM_EPOCHS = os.environ.get(\"NN_DROP_NUM_EPOCHS\", 100 )\n",
    "NN_DROP_LR = os.environ.get(\"NN_DROP_LR\", .005 )\n",
    "\n",
    "KF_TRAIN_NUM_EPOCHS = os.environ.get(\"KF_TRAIN_NUM_EPOCHS\", 25 if METER_TYPE=='electricity' else 10 )\n",
    "KF_TRAIN_LR = os.environ.get(\"KF_TRAIN_LR\", .01)\n",
    "\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT, \"models\", METER_TYPE)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    df_train_clean = pd.read_feather(os.path.join(PROJECT_ROOT, \"clean-data\", \"df_train_clean.feather\"))\n",
    "except Exception:\n",
    "    from clean_data import df_train_clean\n",
    "\n",
    "from prepare_dataset import (\n",
    "    season_config, colname_config, primary_uses, holidays, dataloader_factory\n",
    ")\n",
    "\n",
    "def loss_plot(df_loss: pd.DataFrame):\n",
    "    return (\n",
    "            ggplot(pd.DataFrame(df_loss), aes(x='epoch', y='value')) +\n",
    "            stat_summary(fun_y=np.mean, geom='line') + facet_wrap(\"~dataset\", scales='free') +\n",
    "            theme_bw() + theme(figure_size=(10, 4))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Preparing {METER_TYPE} dataset...\")\n",
    "\n",
    "df_mt_trainval = df_train_clean.\\\n",
    "    loc[df_train_clean['meter'] == METER_TYPE,:].\\\n",
    "    loc[:,['building_id', 'timestamp', 'meter_reading', 'meter_reading_clean', 'lower_thresh']].\\\n",
    "    reset_index(drop=True).\\\n",
    "    assign(\n",
    "        is_drop=lambda df: (df['meter_reading'] < df['lower_thresh']).astype('float'),\n",
    "        meter_reading_clean_pp=lambda df:  np.log1p(df['meter_reading_clean']) # will be modified in next step\n",
    "    )\n",
    "\n",
    "# don't train on buildings (a) if geometric mean is less than ~10, (b) if very few unique values:\n",
    "train_ids = df_mt_trainval.\\\n",
    "    assign(_log1p_mean = lambda df: df.groupby('building_id')['meter_reading_clean_pp'].transform('mean'),\n",
    "           _nunique = lambda df: df.groupby('building_id')['meter_reading_clean'].transform('nunique')).\\\n",
    "    query(\"(_log1p_mean > 2.5) & (_nunique > 50)\").\\\n",
    "    loc[:,'building_id'].\\\n",
    "    drop_duplicates().\\\n",
    "    sample(frac=.80).tolist()\n",
    "val_ids = [id for id in df_mt_trainval['building_id'].drop_duplicates() if id not in train_ids]\n",
    "print(f\"Training {len(train_ids)}, validation {len(val_ids)}\")\n",
    "\n",
    "mt_scaler = DataFrameScaler(\n",
    "        value_colnames=['meter_reading_clean_pp'],\n",
    "        group_colname='building_id',\n",
    "        center=True,\n",
    "        scale=True\n",
    ").fit(df_mt_trainval)\n",
    "\n",
    "df_mt_trainval = mt_scaler.transform(df_mt_trainval, keep_cols='all')\n",
    "\n",
    "dl_train = dataloader_factory(\n",
    "    df_readings=df_mt_trainval.loc[df_mt_trainval['building_id'].isin(train_ids)],\n",
    "    batch_size=50,\n",
    "    drop_colname=False\n",
    ")\n",
    "dl_val = dataloader_factory(\n",
    "    df_readings=df_mt_trainval.loc[df_mt_trainval['building_id'].isin(val_ids)],\n",
    "    batch_size=100,\n",
    "    drop_colname=False\n",
    ")\n",
    "\n",
    "print(\"...finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataloader_factory.predictors)\n",
    "pretrain_nn_module = MultiSeriesStateNN(\n",
    "    num_series=dataloader_factory.df_meta_preds['building_id'].max() + 1, \n",
    "    num_predictors=len(dataloader_factory.predictors),\n",
    "    hidden=(50,25,15),\n",
    "    embed_inputs={\n",
    "        dataloader_factory.predictors.index('primary_use') : {\n",
    "            'num_embeddings' : len(primary_uses), \n",
    "            'embedding_dim' : 3\n",
    "        },\n",
    "        dataloader_factory.predictors.index('holiday') : {\n",
    "            'num_embeddings' : len(holidays) + 1, \n",
    "            'embedding_dim' : 2\n",
    "        }\n",
    "    }\n",
    ")\n",
    "if METER_TYPE != 'electricity':\n",
    "    sd = torch.load(os.path.join(MODEL_DIR, \"../electricity\", \"pretrain_nn_module_state_dict.pkl\"))\n",
    "    pretrain_nn_module.load_state_dict(sd)\n",
    "    print(\"Will initialize pretrain_nn_module w/electricity weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_nn_module.optimizer = torch.optim.Adam(pretrain_nn_module.parameters(), lr=NN_PRETRAIN_LR)\n",
    "pretrain_nn_module.df_loss = []\n",
    "\n",
    "try:\n",
    "    pretrain_nn_module.load_state_dict(torch.load(f\"{MODEL_DIR}/pretrain_nn_module_state_dict.pkl\"))\n",
    "    NN_PRETRAIN_NUM_EPOCHS = 0\n",
    "except FileNotFoundError:\n",
    "    print(f\"Pre-training NN-module for {NN_PRETRAIN_NUM_EPOCHS} epochs...\")\n",
    "    \n",
    "for epoch in range(NN_PRETRAIN_NUM_EPOCHS):\n",
    "    for i, (tb, vb) in enumerate(zip_longest(dl_train, dl_val)):\n",
    "        for nm, batch in {'train' : tb, 'val' : vb}.items():\n",
    "            if not batch:\n",
    "                continue\n",
    "            batch = batch.split_measures(slice(1), slice(1, None))\n",
    "            \n",
    "            y, X = batch.tensors\n",
    "            X[torch.isnan(X)] = 0.0\n",
    "            y = y.squeeze(-1) # batches support multivariate, but we want to squeeze the singleton dim\n",
    "            with torch.set_grad_enabled(nm == 'train'):\n",
    "                prediction = pretrain_nn_module(X, series_idx=batch.group_names if nm == 'train' else None)\n",
    "                loss = torch.mean( (prediction[y == y] - y[y == y]) ** 2 )\n",
    "                \n",
    "            if nm == 'train':\n",
    "                loss.backward()\n",
    "                pretrain_nn_module.optimizer.step()\n",
    "                pretrain_nn_module.optimizer.zero_grad()\n",
    "            pretrain_nn_module.df_loss.append({'value' : loss.item(), 'dataset' : nm, 'epoch' : epoch})\n",
    "            print(f\"batch {i}, {nm} loss {loss.item():.3f}\")\n",
    "            \n",
    "    clear_output(wait=True)        \n",
    "    print(loss_plot(pretrain_nn_module.df_loss) + ggtitle(f\"Epoch {epoch}\"))\n",
    "        \n",
    "    torch.save(pretrain_nn_module.state_dict(), f\"{MODEL_DIR}/pretrain_nn_module_state_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl_train)).split_measures(slice(1), slice(1, None))\n",
    "_, X = batch.tensors\n",
    "X[torch.isnan(X)] = 0.0\n",
    "df_example = batch.tensor_to_dataframe(\n",
    "    tensor=pretrain_nn_module(X, series_idx=batch.group_names).unsqueeze(-1),\n",
    "    times=batch.times(),\n",
    "    group_names=batch.group_names,\n",
    "    group_colname='building_id',\n",
    "    time_colname='timestamp',\n",
    "    measures=['prediction']\n",
    ").query(\"building_id == building_id.sample().item()\").merge(df_mt_trainval)\n",
    "print(\n",
    "    ggplot(df_example.query(\"timestamp.dt.month > 6\"), aes(x='timestamp')) +\n",
    "    geom_line(aes(y='meter_reading_clean_pp')) +\n",
    "    geom_line(aes(y='prediction'), color='red', alpha=.60, size=1.5) +\n",
    "    theme(figure_size=(12,5)) +\n",
    "    ggtitle(str(df_example['building_id'].unique().item()))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_nn_module = MultiSeriesStateNN(\n",
    "    num_series=dataloader_factory.df_meta_preds['building_id'].max() + 1,\n",
    "    **pretrain_nn_module._init_kwargs\n",
    ")\n",
    "for param_name, from_param in pretrain_nn_module.named_parameters():\n",
    "    to_param = dict(drop_nn_module.named_parameters())[param_name]\n",
    "    to_param.data[:] = from_param.data[:]\n",
    "    \n",
    "drop_nn_module.loss_fun = torch.nn.BCELoss()\n",
    "\n",
    "#drop_nn_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_drops = dataloader_factory(\n",
    "    df_readings=df_mt_trainval,\n",
    "    batch_size=50,\n",
    "    drop_colname='is_drop',\n",
    "    reading_colname=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drop_nn_module.optimizer = torch.optim.Adam(drop_nn_module.parameters(), lr=NN_DROP_LR)\n",
    "drop_nn_module.df_loss = []\n",
    "\n",
    "try:\n",
    "    drop_nn_module.load_state_dict(torch.load(f\"{MODEL_DIR}/drop_nn_modxule_state_dict.pkl\"))\n",
    "    NN_DROP_NUM_EPOCHS = 0\n",
    "except FileNotFoundError:\n",
    "    print(f\"Pre-training NN-module for {NN_DROP_NUM_EPOCHS} epochs...\")\n",
    "    \n",
    "for epoch in range(NN_DROP_NUM_EPOCHS):\n",
    "    for i, batch in enumerate(dl_drops):\n",
    "        batch = batch.split_measures(slice(1), slice(1, None))\n",
    "\n",
    "        y, X = batch.tensors\n",
    "        X[torch.isnan(X)] = 0.0\n",
    "        y = y.squeeze(-1) # batches support multivariate, but we want to squeeze the singleton dim\n",
    "        prediction = drop_nn_module(X, series_idx=batch.group_names)\n",
    "\n",
    "        y_train, y_val = y[:,:-1000], y[:,-1000:]\n",
    "        pred_train, pred_val = (torch.sigmoid(x) for x in [prediction[:,:-1000], prediction[:,-1000:]])\n",
    "\n",
    "        loss = drop_nn_module.loss_fun(input=pred_train[y_train==y_train], target=y_train[y_train==y_train])\n",
    "        loss_val = drop_nn_module.loss_fun(input=pred_val[y_val==y_val], target=y_val[y_val==y_val])\n",
    "\n",
    "        loss.backward()\n",
    "        drop_nn_module.optimizer.step()\n",
    "        drop_nn_module.optimizer.zero_grad()\n",
    "        drop_nn_module.df_loss.append({'value' : loss.item(), 'dataset' : 'train', 'epoch' : epoch})\n",
    "        drop_nn_module.df_loss.append({'value' : loss_val.item(), 'dataset' : 'val', 'epoch' : epoch})\n",
    "        print(f\"batch {i}, train loss {loss.item():.3f}\")\n",
    "\n",
    "    clear_output(wait=True)        \n",
    "    print(loss_plot(drop_nn_module.df_loss) + ggtitle(f\"Epoch {epoch}\"))\n",
    "        \n",
    "    torch.save(drop_nn_module.state_dict(), f\"{MODEL_DIR}/drop_nn_module_state_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl_drops)).split_measures(slice(1), slice(1, None))\n",
    "_, X = batch.tensors\n",
    "X[torch.isnan(X)] = 0.0\n",
    "df_example = batch.tensor_to_dataframe(\n",
    "    tensor=torch.sigmoid(drop_nn_module(X, series_idx=batch.group_names).unsqueeze(-1)),\n",
    "    times=batch.times(),\n",
    "    group_names=batch.group_names,\n",
    "    group_colname='building_id',\n",
    "    time_colname='timestamp',\n",
    "    measures=['prediction']\n",
    ").query(\"building_id == building_id.sample().item()\").merge(df_mt_trainval)\n",
    "\n",
    "print(\n",
    "    ggplot(df_example.query(\"timestamp.dt.month > 6\"), aes(x='timestamp')) +\n",
    "    stat_summary_bin(aes(y='is_drop'), fun_y=np.mean, geom='line') +\n",
    "    geom_hline(yintercept=0.) +\n",
    "    geom_line(aes(y='prediction'), color='red', alpha=.60, size=1.5) +\n",
    "    theme(figure_size=(12,5)) +\n",
    "    ggtitle(str(df_example['building_id'].unique().item()))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nn_module = TimeSeriesStateNN(**pretrain_nn_module._init_kwargs)\n",
    "for to_param, from_param in zip(pred_nn_module.parameters(), pretrain_nn_module.parameters()):\n",
    "    to_param.data[:] = from_param.data[:]\n",
    "\n",
    "# output real-values:\n",
    "assert isinstance(pred_nn_module.sequential[-1], torch.nn.Tanh)\n",
    "del pred_nn_module.sequential[-1]\n",
    "\n",
    "pred_nn_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = Forecaster(\n",
    "    processes=[\n",
    "        LocalLevel('level').add_measure('meter_reading_clean_pp'),\n",
    "        \n",
    "        LocalLevel('local_level', decay=(.90,.999)).add_measure('meter_reading_clean_pp'),\n",
    "        \n",
    "        FourierSeasonDynamic(\n",
    "            id='hour_in_day', \n",
    "            seasonal_period=24, \n",
    "            K=2, \n",
    "            decay=(.90,.999), \n",
    "            **season_config\n",
    "        ).add_measure('meter_reading_clean_pp'),\n",
    "        \n",
    "        NN(\n",
    "            id='predictors', \n",
    "            input_dim=len(dataloader_factory.predictors), \n",
    "            state_dim=pred_nn_module.sequential[-1].out_features,\n",
    "            nn_module=pred_nn_module,\n",
    "            add_module_params_to_process=False\n",
    "        ).add_measure('meter_reading_clean_pp')\n",
    "    ],\n",
    "     measures=['meter_reading_clean_pp']\n",
    "    )\n",
    "\n",
    "# better init:\n",
    "if METER_TYPE == 'electricity':\n",
    "    kf.design.process_covariance.set(kf.design.process_covariance.create().data / 100.)\n",
    "else:\n",
    "    pred_nn_module.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"../electricity\", \"pred_nn_module_state_dict.pkl\")))\n",
    "    kf.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"../electricity\", \"kf_state_dict.pkl\")))\n",
    "    print(\"Will initialize KF w/electricity weights.\")\n",
    "\n",
    "# optimizer:\n",
    "kf.optimizer = torch.optim.Adam(kf.parameters(), lr=KF_TRAIN_LR)\n",
    "kf.optimizer.add_param_group({'params' : pred_nn_module.parameters(), 'lr' : KF_TRAIN_LR / 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    kf.load_state_dict(torch.load(f\"{MODEL_DIR}/kf_state_dict.pkl\"))\n",
    "    pred_nn_module.load_state_dict(torch.load(f\"{MODEL_DIR}/pred_nn_module_state_dict.pkl\"))\n",
    "    KF_TRAIN_NUM_EPOCHS = 0\n",
    "except FileNotFoundError:\n",
    "    print(f\"Training KF for {KF_TRAIN_NUM_EPOCHS} epochs...\")\n",
    "\n",
    "kf.df_loss = []\n",
    "for epoch in range(KF_TRAIN_NUM_EPOCHS):\n",
    "    for i, (tb, vb) in enumerate(zip_longest(dl_train, dl_val)):\n",
    "        for nm, batch in {'train' : tb, 'val' : vb}.items():\n",
    "            if not batch:\n",
    "                continue\n",
    "            batch = batch.split_measures(slice(1), slice(1, None))\n",
    "            batch.tensors[1][torch.isnan(batch.tensors[1])] = 0.0 \n",
    "            with torch.set_grad_enabled(nm == 'train'):\n",
    "                loss = kf.forward_backward(\n",
    "                    batch=batch, \n",
    "                    delete_interval='60D', \n",
    "                    random_state=rs if nm == 'val' else None\n",
    "                )\n",
    "            if nm == 'train':\n",
    "                kf.optimizer.zero_grad()\n",
    "            kf.df_loss.append({'value' : loss.item(), 'dataset' : nm, 'epoch' : epoch})\n",
    "            print(f\"{nm} batch {i} loss {loss.item():.3f}\")\n",
    "    clear_output(wait=True)\n",
    "    print(loss_plot(kf.df_loss) + ggtitle(f\"Epoch {epoch}\"))\n",
    "    \n",
    "    torch.save(kf.state_dict(), f\"{MODEL_DIR}/kf_state_dict.pkl\")\n",
    "    torch.save(pred_nn_module.state_dict(), f\"{MODEL_DIR}/pred_nn_module_state_dict.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_forecast_dt = np.datetime64('2016-06-01')\n",
    "\n",
    "df_val_forecast = []\n",
    "for batch in dl_val:\n",
    "    batch = batch.split_measures(slice(1), slice(1, None))\n",
    "    with torch.no_grad():\n",
    "        readings, predictors = (t.clone() for t in batch.tensors)\n",
    "        readings[np.where(batch.times() > val_forecast_dt)] = float('nan')\n",
    "        predictors[torch.isnan(predictors)] = 0.\n",
    "        pred = kf(\n",
    "            readings,\n",
    "            start_datetimes=batch.start_datetimes,\n",
    "            predictors=predictors,\n",
    "            progress=True\n",
    "        )\n",
    "    df = pred.to_dataframe(batch, **colname_config).\\\n",
    "      query(\"measure == 'meter_reading_clean_pp'\").\\\n",
    "      drop(columns=['measure'])\n",
    "    df_val_forecast.append(df)\n",
    "df_val_forecast = pd.concat(df_val_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_example = df_val_forecast.\\\n",
    "             query(\"(building_id == building_id.sample().item()) & (timestamp.dt.month >= 2)\").\\\n",
    "             merge(df_mt_trainval, how='left')\n",
    "\n",
    "print(\n",
    "    ggplot(df_example, \n",
    "           aes(x='timestamp')) +\n",
    "    geom_line(aes(y='predicted_mean'), color='red', alpha=.50, size=1) +\n",
    "    geom_ribbon(aes(ymin='predicted_mean - predicted_std', ymax='predicted_mean + predicted_std'), alpha=.25) +\n",
    "    geom_line(aes(y='meter_reading_clean_pp')) +\n",
    "    geom_vline(xintercept=np.datetime64('2016-06-01')) +\n",
    "    theme(figure_size=(12,5)) +\n",
    "    ggtitle(str(df_example['building_id'].unique().item()))\n",
    ")\n",
    "\n",
    "print(\n",
    "    ggplot(df_example.assign(train=lambda df: df['timestamp'] < '2016-06-01'), \n",
    "           aes(x='timestamp.dt.hour')) +\n",
    "    stat_summary(aes(y='predicted_mean'), color='red', alpha=.50, size=1) +\n",
    "    stat_summary(aes(y='meter_reading_clean_pp')) +\n",
    "    theme(figure_size=(12,5)) +\n",
    "    facet_wrap(\"~train\") +\n",
    "    ggtitle(str(df_example['building_id'].unique().item()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df_val_err = df_val_forecast.\\\n",
    "              merge(df_mt_trainval, how='left').\\\n",
    "    assign(resid= lambda df: df['predicted_mean'] - df['meter_reading_clean_pp'], # TODO: inverse-transform\n",
    "           mse = lambda df: df['resid'] ** 2,\n",
    "           month = lambda df: df['timestamp'].dt.month).\\\n",
    "    groupby(['building_id','month'])\\\n",
    "    ['mse','resid'].mean().\\\n",
    "    reset_index()\n",
    "\n",
    "print(\n",
    "    ggplot(df_val_err, aes(x='month')) + \n",
    "    stat_summary(aes(y='resid'), fun_data='mean_cl_boot', color='red') +\n",
    "    stat_summary(aes(y='mse'), fun_data='mean_cl_boot', color='blue') +\n",
    "    geom_hline(yintercept=0.0) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"output\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(DATA_DIR,\"test.csv.gz\"), parse_dates=['timestamp'])\n",
    "df_test['meter'] = df_test['meter'].map(meter_mapping).astype('category')\n",
    "df_mt_test = pd.concat([\n",
    "    df_mt_trainval.\\\n",
    "        loc[:,['building_id','timestamp','meter_reading_clean_pp','meter_reading','is_drop']].\\\n",
    "        assign(row_id=-1).\\\n",
    "        reset_index(drop=True),\n",
    "    df_test.\\\n",
    "        loc[df_test['meter']==METER_TYPE,:].\\\n",
    "        drop(columns=['meter']).\\\n",
    "        reset_index(drop=True)\n",
    "], sort=True)\n",
    "\n",
    "df_mt_test['chunk'] = np.floor(df_mt_test['building_id'] / 50).astype('int')\n",
    "\n",
    "for chunk, df_in in df_mt_test.groupby('chunk'):\n",
    "    dl = dataloader_factory(df_in, reading_colname='meter_reading_clean_pp', drop_colname=False, batch_size=50)\n",
    "    assert len(dl) == 1\n",
    "    batch = next(iter(dl)).split_measures(slice(1), slice(1, None))\n",
    "    with torch.no_grad():\n",
    "        readings, predictors = (t.clone() for t in batch.tensors)\n",
    "        predictors[torch.isnan(predictors)] = 0.\n",
    "        pred = kf(\n",
    "            readings,\n",
    "            start_datetimes=batch.start_datetimes,\n",
    "            predictors=predictors,\n",
    "            progress=tqdm_notebook\n",
    "        )\n",
    "        drop_pred = torch.sigmoid(drop_nn_module(predictors, batch.group_names))\n",
    "\n",
    "    df_pred = pred.to_dataframe(batch, **colname_config).\\\n",
    "      query(\"measure == 'meter_reading_clean_pp'\").\\\n",
    "      drop(columns=['measure']).\\\n",
    "      merge(\n",
    "        TimeSeriesDataset.tensor_to_dataframe(\n",
    "            tensor=drop_pred.unsqueeze(-1), \n",
    "            times=batch.times(), \n",
    "            group_names=batch.group_names, \n",
    "            **colname_config, \n",
    "            measures=['predicted_drop_prob']\n",
    "            )\n",
    "        ).\\\n",
    "        merge(df_in.loc[:,['building_id', 'timestamp', 'row_id', 'meter_reading','is_drop']], \n",
    "              how='left',\n",
    "              on=['building_id', 'timestamp'])\n",
    "    \n",
    "    df_pred = df_pred.merge(\n",
    "        df_pred.\\\n",
    "            query('is_drop==1').\\\n",
    "            assign(meter_reading_log1p=lambda df: np.log1p(df['meter_reading'])).\\\n",
    "            groupby('building_id')\\\n",
    "            ['meter_reading_log1p'].mean().\\\n",
    "            reset_index().\\\n",
    "            assign(avg_when_dropped = lambda df: np.expm1(df.pop('meter_reading_log1p'))),\n",
    "        how='left'\n",
    "        ).\\\n",
    "        rename(columns={'predicted_mean' : 'meter_reading_clean_pp'}).\\\n",
    "        pipe(mt_scaler.inverse_transform, keep_cols=['row_id','predicted_drop_prob','avg_when_dropped', 'meter_reading'])\n",
    "    \n",
    "    df_pred['_meter_reading'] = \\\n",
    "        np.expm1(df_pred['meter_reading_clean_pp']) * (1-df_pred['predicted_drop_prob']) + \\\n",
    "        df_pred['avg_when_dropped'].fillna(0.0) * (df_pred['predicted_drop_prob']) \n",
    "    \n",
    "    sq_err = (np.log1p(df_pred['_meter_reading']) - np.log1p(df_pred['meter_reading'])) ** 2\n",
    "    print(f\"MSE: {sq_err.mean():.3f}\")\n",
    "    \n",
    "    df_out = df_pred.\\\n",
    "      assign(meter_reading=lambda df: df['_meter_reading']).\\\n",
    "      query(\"row_id >= 0\").\\\n",
    "      loc[:,['row_id', 'meter_reading']].\\\n",
    "      reset_index(drop=True)\n",
    "\n",
    "    path = os.path.join(OUTPUT_DIR, f\"{METER_TYPE}-{chunk}.feather\")\n",
    "    print(path)\n",
    "    df_out.to_feather(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
